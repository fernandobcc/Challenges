{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>HTTP requests to the NASA Kennedy Space Center WWW server </h3> \n",
    " \n",
    "<p>Fonte oficial do dateset: http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html \n",
    "Dados:</p>\n",
    "<ul>\n",
    "<li>Jul 01 to Jul 31, ASCII format, 20.7 MB gzip compressed, 205.2 MB. </li>\n",
    "<li>Aug 04 to Aug 31, ASCII format, 21.8 MB gzip compressed, 167.8 MB. </li> \n",
    "</ul>\n",
    "\n",
    "<p><b>Sobre o dataset:</b> Esses dois conjuntos de dados possuem todas as requisições HTTP para o servidor da NASA Kennedy Space Center WWW na Flórida para um período específico. \n",
    "Os logs estão em arquivos ASCII com uma linha por requisição com as seguintes colunas: </p> \n",
    "<ul>\n",
    "<li> <b>Host fazendo a requisição. </b> Um hostname quando possível, caso contrário o endereço de internet se o nome não puder ser identificado. </li> \n",
    "<li> <b>Timestamp</b> no formato \"DIA/MÊS/ANO:HH:MM:SS TIMEZONE\" </li>\n",
    "<li> <b>Requisição</b> (entre aspas) </li>\n",
    "<li> <b>Código do retorno HTTP</b>  </li>\n",
    "<li> <b>Total de bytes retornados </b></li> \n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import udf, to_date, to_utc_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASO NECESSÁRIO\n",
    "#!pip install --upgrade wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NASA_access_log_Aug95.gz',\n",
       " 'v0.2.20.tar.gz',\n",
       " 'NASA_access_log_Jul95.gz',\n",
       " 'NASA_access_log_Aug95 (1).gz',\n",
       " 'NASA_access_log_Jul95 (1).gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_files = glob.glob('*.gz')\n",
    "raw_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import glob\n",
    "\n",
    "#Em caso de ainda não tiver o dataset\n",
    "#Obtendo os dados \n",
    "#data1 = 'ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz'\n",
    "#data2 = 'ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz'\n",
    "#file1 = wget.download(data1)\n",
    "#file2 = wget.download(data2)\n",
    "\n",
    "#Caso já tenha o data set\n",
    "arquivos_gz = glob.glob('*.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NASA_access_log_Aug95.gz',\n",
       " 'v0.2.20.tar.gz',\n",
       " 'NASA_access_log_Jul95.gz',\n",
       " 'NASA_access_log_Aug95 (1).gz',\n",
       " 'NASA_access_log_Jul95 (1).gz']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Arquivos .gz\n",
    "arquivos_gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = arquivos_gz[2] #NASA_access_log_Jul95.gz'\n",
    "file2 = arquivos_gz[0] #NASA_access_log_Aug95.gz'\n",
    "\n",
    "#Criando vertor de arquivos\n",
    "file = [file1,file2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245'),\n",
       " Row(value='unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985'),\n",
       " Row(value='199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085'),\n",
       " Row(value='burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0'),\n",
       " Row(value='199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179'),\n",
       " Row(value='burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0'),\n",
       " Row(value='burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0'),\n",
       " Row(value='205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985'),\n",
       " Row(value='d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985'),\n",
       " Row(value='129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#criando dataframe\n",
    "df = spark.read.text(file)\n",
    "#Criando RDD\n",
    "df_rdd = df.rdd\n",
    "df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraindo_dados(data):\n",
    "        '''Esta método será responsável por gerar um dataframe com a separação correta dos dados de acordo com as equações regulares'''\n",
    "        \n",
    "        padrao_host = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "        padrao_timestamp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "        padrao_end =  r'^.*\"\\w+\\s+([^\\s]+)\\s+HTTP.*\"'\n",
    "        padrao_status = r'\\s(\\d{3})\\s'\n",
    "        padrao_tamanho_cont = r'\\s(\\d+)$'\n",
    "        df_logs = data.select(regexp_extract('value', padrao_host, 1).alias('host'),\n",
    "                         regexp_extract('value', padrao_timestamp, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', padrao_end, 1).alias('caminho'),\n",
    "                         regexp_extract('value', padrao_status, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', padrao_tamanho_cont, 1).cast('integer').alias('tam_conteudo'))\n",
    "        return df_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+------------+\n",
      "|                host|           timestamp|             caminho|status|tam_conteudo|\n",
      "+--------------------+--------------------+--------------------+------+------------+\n",
      "|        199.72.81.55|01/Jul/1995:00:00...|    /history/apollo/|   200|        6245|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...| /shuttle/countdown/|   200|        3985|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|/shuttle/missions...|   200|        4085|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|/shuttle/countdow...|   304|           0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|/shuttle/missions...|   200|        4179|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|/images/NASA-logo...|   304|           0|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|/shuttle/countdow...|   200|           0|\n",
      "|     205.212.115.106|01/Jul/1995:00:00...|/shuttle/countdow...|   200|        3985|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...| /shuttle/countdown/|   200|        3985|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|                   /|   200|        7074|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|/shuttle/countdow...|   200|       40310|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|/images/NASA-logo...|   200|         786|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|/images/KSC-logos...|   200|        1204|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|/shuttle/countdow...|   200|       40310|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|/images/NASA-logo...|   200|         786|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|/images/KSC-logos...|   200|        1204|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|/images/ksclogo-m...|   304|           0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|/images/launch-lo...|   200|        1713|\n",
      "|ppptky391.asahi-n...|01/Jul/1995:00:00...|/facts/about_ksc....|   200|        3977|\n",
      "|  net-1-141.eden.com|01/Jul/1995:00:00...|/shuttle/missions...|   200|       34029|\n",
      "+--------------------+--------------------+--------------------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_logs = extraindo_dados(df)\n",
    "df_logs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55', timestamp='01/Jul/1995:00:00:01 -0400', caminho='/history/apollo/', status=200, tam_conteudo=6245),\n",
       " Row(host='unicomp6.unicomp.net', timestamp='01/Jul/1995:00:00:06 -0400', caminho='/shuttle/countdown/', status=200, tam_conteudo=3985),\n",
       " Row(host='199.120.110.21', timestamp='01/Jul/1995:00:00:09 -0400', caminho='/shuttle/missions/sts-73/mission-sts-73.html', status=200, tam_conteudo=4085),\n",
       " Row(host='burger.letters.com', timestamp='01/Jul/1995:00:00:11 -0400', caminho='/shuttle/countdown/liftoff.html', status=304, tam_conteudo=0),\n",
       " Row(host='199.120.110.21', timestamp='01/Jul/1995:00:00:11 -0400', caminho='/shuttle/missions/sts-73/sts-73-patch-small.gif', status=200, tam_conteudo=4179)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Permite usar comandos SQL para trabalhar com spark dataframe\n",
    "df_logs.createOrReplaceTempView(\"logs\")\n",
    "spark.sql(\"SELECT * FROM logs\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3461613|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verificando se há valores nulos\n",
    "spark.sql(\"SELECT COUNT(*) FROM logs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+------+------------+\n",
      "|host|timestamp|caminho|status|tam_conteudo|\n",
      "+----+---------+-------+------+------------+\n",
      "|   0|        0|      0|     1|       33905|\n",
      "+----+---------+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verifica quais colunas tem valores nulos\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "COUNT(*)-COUNT(host) As host,\n",
    "COUNT(*)-COUNT(timestamp) As timestamp,\n",
    "COUNT(*)-COUNT(caminho) As caminho,\n",
    "COUNT(*)-COUNT(status) As status,\n",
    "COUNT(*)-COUNT(tam_conteudo) As tam_conteudo\n",
    "FROM logs\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subistituindo os valores nulos por 0\n",
    "df_logs_atual= df_logs.na.fill({'tam_conteudo': 0, 'status':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(host='199.72.81.55', timestamp='01/Jul/1995:00:00:01 -0400', caminho='/history/apollo/', status=200, tam_conteudo=6245),\n",
       " Row(host='unicomp6.unicomp.net', timestamp='01/Jul/1995:00:00:06 -0400', caminho='/shuttle/countdown/', status=200, tam_conteudo=3985),\n",
       " Row(host='199.120.110.21', timestamp='01/Jul/1995:00:00:09 -0400', caminho='/shuttle/missions/sts-73/mission-sts-73.html', status=200, tam_conteudo=4085),\n",
       " Row(host='burger.letters.com', timestamp='01/Jul/1995:00:00:11 -0400', caminho='/shuttle/countdown/liftoff.html', status=304, tam_conteudo=0),\n",
       " Row(host='199.120.110.21', timestamp='01/Jul/1995:00:00:11 -0400', caminho='/shuttle/missions/sts-73/sts-73-patch-small.gif', status=200, tam_conteudo=4179)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_logs_atual.createOrReplaceTempView(\"logs\")\n",
    "spark.sql(\"SELECT * FROM logs\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+------+------------+\n",
      "|host|timestamp|caminho|status|tam_conteudo|\n",
      "+----+---------+-------+------+------------+\n",
      "|   0|        0|      0|     0|           0|\n",
      "+----+---------+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Verifica novamente a presença de nulos\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "COUNT(*)-COUNT(host) As host,\n",
    "COUNT(*)-COUNT(timestamp) As timestamp,\n",
    "COUNT(*)-COUNT(caminho) As caminho,\n",
    "COUNT(*)-COUNT(status) As status,\n",
    "COUNT(*)-COUNT(tam_conteudo) As tam_conteudo\n",
    "FROM logs\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria-se uma função para estrair/converter a data do timestamp (os dias serão importante para responder as questões)\n",
    "def converte_para_timestamp(date):\n",
    "        ''' Converte a string de data/time para data'''\n",
    "        try:\n",
    "            return datetime.strptime(date.split(':')[0], \"%d/%b/%Y\").strftime(\"%Y-%m-%d\")\n",
    "        except Exception as e:\n",
    "            return 0\n",
    "\n",
    "\n",
    "converte_data = udf(converte_para_timestamp)\n",
    "df_final = df_logs_atual.select('*', converte_data (df_logs_atual['timestamp']).cast('timestamp').alias('time')).drop('timestamp')\n",
    "total_log_entries = df_final.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- caminho: string (nullable = true)\n",
      " |-- status: integer (nullable = false)\n",
      " |-- tam_conteudo: integer (nullable = false)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dataframe após alterações\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+------------+--------------------+\n",
      "|                host|             caminho|status|tam_conteudo|                time|\n",
      "+--------------------+--------------------+------+------------+--------------------+\n",
      "|        199.72.81.55|    /history/apollo/|   200|        6245|1995-07-01 00:00:...|\n",
      "|unicomp6.unicomp.net| /shuttle/countdown/|   200|        3985|1995-07-01 00:00:...|\n",
      "|      199.120.110.21|/shuttle/missions...|   200|        4085|1995-07-01 00:00:...|\n",
      "|  burger.letters.com|/shuttle/countdow...|   304|           0|1995-07-01 00:00:...|\n",
      "|      199.120.110.21|/shuttle/missions...|   200|        4179|1995-07-01 00:00:...|\n",
      "|  burger.letters.com|/images/NASA-logo...|   304|           0|1995-07-01 00:00:...|\n",
      "|  burger.letters.com|/shuttle/countdow...|   200|           0|1995-07-01 00:00:...|\n",
      "|     205.212.115.106|/shuttle/countdow...|   200|        3985|1995-07-01 00:00:...|\n",
      "|         d104.aa.net| /shuttle/countdown/|   200|        3985|1995-07-01 00:00:...|\n",
      "|      129.94.144.152|                   /|   200|        7074|1995-07-01 00:00:...|\n",
      "|unicomp6.unicomp.net|/shuttle/countdow...|   200|       40310|1995-07-01 00:00:...|\n",
      "|unicomp6.unicomp.net|/images/NASA-logo...|   200|         786|1995-07-01 00:00:...|\n",
      "|unicomp6.unicomp.net|/images/KSC-logos...|   200|        1204|1995-07-01 00:00:...|\n",
      "|         d104.aa.net|/shuttle/countdow...|   200|       40310|1995-07-01 00:00:...|\n",
      "|         d104.aa.net|/images/NASA-logo...|   200|         786|1995-07-01 00:00:...|\n",
      "|         d104.aa.net|/images/KSC-logos...|   200|        1204|1995-07-01 00:00:...|\n",
      "|      129.94.144.152|/images/ksclogo-m...|   304|           0|1995-07-01 00:00:...|\n",
      "|      199.120.110.21|/images/launch-lo...|   200|        1713|1995-07-01 00:00:...|\n",
      "|ppptky391.asahi-n...|/facts/about_ksc....|   200|        3977|1995-07-01 00:00:...|\n",
      "|  net-1-141.eden.com|/shuttle/missions...|   200|       34029|1995-07-01 00:00:...|\n",
      "+--------------------+--------------------+------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.cache() #Cria-se cache pois essa dataframe será usado mais vezes \n",
    "df_final.createOrReplaceTempView(\"logs\") #Permite o uso de comandos SQL\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Número de hosts únicos. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hosts unicos: 137933\n"
     ]
    }
   ],
   "source": [
    "# Obtem o total de host unicos\n",
    "total_de_host_unicos = df_final.select('host').distinct().count()\n",
    "#OU (caso queira usar SQL)\n",
    "#total_de_host_unicos = spark.sql('SELECT DISTINCT host FROM logs').count()\n",
    "\n",
    "#IMPRIME RESULTADO\n",
    "print('Hosts unicos: {0}'.format(total_de_host_unicos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. O total de erros 404. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seleciona todos erros 404 \n",
    "erros_404 = df_final.filter(df_final.status == 404).cache()\n",
    "#OU (caso queira usar SQL)\n",
    "#erros_404 = spark.sql('SELECT * FROM logs WHERE status = 404').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de erros 404: 20899\n"
     ]
    }
   ],
   "source": [
    "total_404 = erros_404.count()\n",
    "print('Total de erros 404: {0}'.format(total_404))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Os 5 URLs que mais causaram erro 404.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtem o top 5 URLs que causam mais problemas\n",
    "top5_erros_404 = erros_404.groupBy(\"host\").count().sort(\"count\", ascending=False).limit(5)\n",
    "#OU (caso queira usar SQL)\n",
    "#top5_erros_404 = spark.sql('SELECT host, count(*) AS count FROM logs WHERE status = 404 GROUP BY host ORDER BY count DESC LIMIT 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n",
      "|host                       |count|\n",
      "+---------------------------+-----+\n",
      "|hoohoo.ncsa.uiuc.edu       |251  |\n",
      "|piweba3y.prodigy.com       |157  |\n",
      "|jbiagioni.npt.nuwc.navy.mil|132  |\n",
      "|piweba1y.prodigy.com       |114  |\n",
      "|                           |112  |\n",
      "+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_erros_404.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Quantidade de erros 404 por dia. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtem a quantidade por dia dos erros\n",
    "erros_404_dias = (erros_404.groupBy(F.dayofmonth('time').alias('day')).count().sort(\"day\"))\n",
    "#OU (caso queira usar SQL)\n",
    "#erros_404_dias = spark.sql('SELECT DAY(time) AS day, count(*) AS count FROM logs WHERE status = 404 GROUP BY DAY(time) ORDER BY DAY(time) ASC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|day|count|\n",
      "+---+-----+\n",
      "|  1|  559|\n",
      "|  2|  291|\n",
      "|  3|  778|\n",
      "|  4|  705|\n",
      "|  5|  733|\n",
      "|  6| 1013|\n",
      "|  7| 1107|\n",
      "|  8|  691|\n",
      "|  9|  627|\n",
      "| 10|  713|\n",
      "| 11|  734|\n",
      "| 12|  667|\n",
      "| 13|  748|\n",
      "| 14|  700|\n",
      "| 15|  581|\n",
      "| 16|  516|\n",
      "| 17|  677|\n",
      "| 18|  721|\n",
      "| 19|  848|\n",
      "| 20|  740|\n",
      "| 21|  639|\n",
      "| 22|  480|\n",
      "| 23|  578|\n",
      "| 24|  748|\n",
      "| 25|  876|\n",
      "| 26|  702|\n",
      "| 27|  706|\n",
      "| 28|  504|\n",
      "| 29|  420|\n",
      "| 30|  571|\n",
      "| 31|  526|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "erros_404_dias.show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. O total de bytes retornados. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bytes = spark.sql('SELECT count(tam_conteudo) AS Qtd_Bytes, sum(tam_conteudo) AS Total_Bytes FROM logs')\n",
    "#OU (caso queira usar SQL)\n",
    "#total_bytes = df_final.agg(F.count(df_final['tam_conteudo']).alias('Qtd_Bytes'), F.sum(df_final['tam_conteudo']).alias('Qtd_Bytes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Qtd_Bytes|Total_Bytes|\n",
      "+---------+-----------+\n",
      "|  3461613|65524314915|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_bytes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
